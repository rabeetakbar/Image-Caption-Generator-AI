{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJm_3tghL7eI",
        "outputId": "4fd7c67d-04ed-44a5-905e-f4b0cb48c6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/rabeet\n"
          ]
        }
      ],
      "source": [
        "#This will mount Google drive to Colab VM\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FolderName = '/rabeet/'\n",
        "assert FolderName is not None, \"[!] Enter the foldername\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/rabeet/data'.format(FolderName))\n",
        "\n",
        "%cd /content/drive/My\\ Drive/$FolderName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-WBpK3kZzHE",
        "outputId": "1cf073dc-a9e5-4fef-f4f9-4be1b4adad90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q3mQ5bkI93Z"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPIpXUnHIxe6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1tpw5LWJKub"
      },
      "source": [
        "# **Define Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM0c9J4UJTow",
        "outputId": "2f2d0d16-3da8-4526-9222-702b715a1d2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download the NLTK tokenizer data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Hyperparameters\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Paths\n",
        "data_dir = 'data'\n",
        "captions_file = os.path.join(data_dir, 'captions.txt')\n",
        "images_dir = os.path.join(data_dir, 'Images')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHt4Unn8JU9a"
      },
      "source": [
        "# **Read Captions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRuesYwNJbOZ"
      },
      "outputs": [],
      "source": [
        "# Read the captions file\n",
        "captions_data = {}\n",
        "with open('/content/drive/MyDrive/rabeet/data/captions.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        img_id, caption = line.strip().split(',', 1)\n",
        "        if img_id not in captions_data:\n",
        "            captions_data[img_id] = []\n",
        "        captions_data[img_id].append(caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUIXBx9SJgRg"
      },
      "source": [
        "# **Vocabulary Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbXK_tHqJmOI"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {v: k for k, v in self.itos.items()}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return word_tokenize(text.lower())\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                frequencies[word] += 1\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XNeqEqeJtXK"
      },
      "source": [
        "# **Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nMQCPyEJzmC"
      },
      "outputs": [],
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_data, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.captions_data = captions_data\n",
        "        self.transform = transform\n",
        "        self.imgs = list(self.captions_data.keys())\n",
        "        self.captions = [caption for captions in self.captions_data.values() for caption in captions]\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_id = self.imgs[index]\n",
        "        caption = self.captions_data[img_id][0]\n",
        "        img_path = os.path.join(self.root_dir, 'Images', img_id)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n",
        "        return img, torch.tensor(numericalized_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktk0iUNfKI9i"
      },
      "source": [
        "# Create data loaders for training, validation, and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cAz2I-kKNZK"
      },
      "outputs": [],
      "source": [
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# Custom collate function to pad captions\n",
        "def collate_fn(batch):\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
        "    return images, captions, lengths\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "dataset = CaptionDataset(root_dir=data_dir, captions_data=captions_data, transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfrEDG4oKQcd"
      },
      "source": [
        "# **Image Feature Extraction**\n",
        "\n",
        "Define the CNN to extract image features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2WJsoGJKphb"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(CNNModel, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad_(False)\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bM4R3RiKsPY"
      },
      "source": [
        "# **Text Generator**\n",
        "\n",
        "Define the decoder (RNN) to generate captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKE_iAN7K2Q2"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions, lengths):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        packed_embeddings = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_hiddens, _ = self.lstm(packed_embeddings)\n",
        "        hiddens, _ = pad_packed_sequence(packed_hiddens, batch_first=True)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k-MPsmPLRhZ"
      },
      "source": [
        "# **Caption Generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijz2n1o1LUl9"
      },
      "outputs": [],
      "source": [
        "class CaptionGenerator(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(CaptionGenerator, self).__init__()\n",
        "        self.encoder = CNNModel(embed_size)\n",
        "        self.decoder = RNNModel(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "    def forward(self, images, captions, lengths):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions, lengths)\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocab, max_length=20):\n",
        "        result_caption = []\n",
        "        with torch.no_grad():\n",
        "            x = self.encoder(image).unsqueeze(0)\n",
        "            states = None\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoder.lstm(x, states)\n",
        "                output = self.decoder.linear(hiddens.squeeze(1))\n",
        "                predicted = output.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoder.embed(predicted).unsqueeze(1)\n",
        "                if vocab.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "        return [vocab.itos[idx] for idx in result_caption]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM3w7GnELZGg",
        "outputId": "5df8b65d-fe77-4f36-89f2-ee6150654ef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:02<00:00, 34.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Training and testing\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vocab_size = len(dataset.vocab)\n",
        "model = CaptionGenerator(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8uPvjlGLcTt"
      },
      "source": [
        "# **Training and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knoGuEJtIysT"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, model_save_path):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            lengths = [length - 1 for length in lengths]  # Adjust lengths for <SOS> token\n",
        "            targets = pack_padded_sequence(captions[:, 1:], lengths, batch_first=True, enforce_sorted=False).data\n",
        "            outputs = model(images, captions, lengths)\n",
        "            outputs = pack_padded_sequence(outputs, lengths, batch_first=True, enforce_sorted=False).data\n",
        "            loss = criterion(outputs, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "        validate_model(model, criterion, val_loader)\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f'Model saved to {model_save_path}')\n",
        "\n",
        "# Validation function\n",
        "def validate_model(model, criterion, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for i, (images, captions, lengths) in enumerate(val_loader):\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            lengths = [length - 1 for length in lengths]  # Adjust lengths for <SOS> token\n",
        "            targets = pack_padded_sequence(captions[:, 1:], lengths, batch_first=True, enforce_sorted=False).data\n",
        "            outputs = model(images, captions, lengths)\n",
        "            outputs = pack_padded_sequence(outputs, lengths, batch_first=True, enforce_sorted=False).data\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        print(f'Validation Loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRPNWSXusGZr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training function\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, model_save_path):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            lengths = [length - 1 for length in lengths]  # Adjust lengths for <SOS> token\n",
        "            targets = pack_padded_sequence(captions[:, 1:], lengths, batch_first=True, enforce_sorted=False).data\n",
        "            outputs = model(images, captions, lengths)\n",
        "            outputs = pack_padded_sequence(outputs, lengths, batch_first=True, enforce_sorted=False).data\n",
        "            loss = criterion(outputs, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_loss, val_accuracy = validate_model(model, criterion, val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f'Model saved to {model_save_path}')\n",
        "\n",
        "    # Plotting the loss and accuracy graphs\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Validation function\n",
        "def validate_model(model, criterion, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, captions, lengths) in enumerate(val_loader):\n",
        "            images, captions = images.to(device), captions.to(device)\n",
        "            lengths = [length - 1 for length in lengths]  # Adjust lengths for <SOS> token\n",
        "            targets = pack_padded_sequence(captions[:, 1:], lengths, batch_first=True, enforce_sorted=False).data\n",
        "            outputs = model(images, captions, lengths)\n",
        "            outputs = pack_padded_sequence(outputs, lengths, batch_first=True, enforce_sorted=False).data\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "# Ensure to import the necessary packages and define your dataset, model, and collate_fn\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# from your_dataset_and_model import ImageCaptionDataset, CaptioningModel\n",
        "\n",
        "# Dummy setup for train and validation data loaders\n",
        "# train_dataset = ImageCaptionDataset(train=True)\n",
        "# val_dataset = ImageCaptionDataset(train=False)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "# model = CaptioningModel().to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define number of epochs and model save path\n",
        "# num_epochs = 10\n",
        "# model_save_path = 'image_caption_model.pth'\n",
        "\n",
        "# Train the model\n",
        "# train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, model_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "EaZAzjm3Lnfn",
        "outputId": "9907d1e9-adad-47cd-e8d8-8ef26257c9c6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3cedfaff0d7b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'image_captioning_model.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "model_save_path = 'image_captioning_model.pth'\n",
        "train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs, model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp5DLXJVoF4-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVP5-2SfLrOb"
      },
      "source": [
        "# **Test the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_OLrDjmLuOX"
      },
      "outputs": [],
      "source": [
        "# Test function\n",
        "def test_model(model, test_loader, vocab):\n",
        "    model.eval()\n",
        "    test_image, test_caption, _ = next(iter(test_loader))\n",
        "    test_image = test_image.to(device)\n",
        "    generated_caption = model.caption_image(test_image[0].unsqueeze(0), vocab)\n",
        "    print('Generated Caption:', ' '.join(generated_caption))\n",
        "    print('Actual Caption:', ' '.join([vocab.itos[idx] for idx in test_caption[0].cpu().numpy()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnInFvIIwS79"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Test function\n",
        "def test_model(model, test_loader, vocab):\n",
        "    model.eval()\n",
        "    test_image, test_caption, _ = next(iter(test_loader))\n",
        "    test_image = test_image.to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    generated_caption = model.caption_image(test_image[0].unsqueeze(0), vocab)\n",
        "\n",
        "    # Convert the actual caption to words\n",
        "    actual_caption = ' '.join([vocab.itos[idx] for idx in test_caption[0].cpu().numpy()])\n",
        "\n",
        "    # Convert the generated caption to words\n",
        "    generated_caption_str = ' '.join(generated_caption)\n",
        "\n",
        "    print('Generated Caption:', generated_caption_str)\n",
        "    print('Actual Caption:', actual_caption)\n",
        "\n",
        "    # Move the image to CPU and convert it to numpy for plotting\n",
        "    image = test_image[0].cpu().permute(1, 2, 0).numpy()\n",
        "\n",
        "    # Plot the image along with the captions\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f'Generated: {generated_caption_str}\\nActual: {actual_caption}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Ensure to import the necessary packages and define your dataset, model, and other components\n",
        "# import torch\n",
        "# from torch.utils.data import DataLoader\n",
        "# from your_dataset_and_model import ImageCaptionDataset, CaptioningModel, Vocabulary\n",
        "\n",
        "# Dummy setup for test data loader\n",
        "# test_dataset = ImageCaptionDataset(train=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize the model and load the trained weights\n",
        "# model = CaptioningModel().to(device)\n",
        "# model.load_state_dict(torch.load('image_caption_model.pth'))\n",
        "\n",
        "# Assuming vocab is defined elsewhere and passed to the test function\n",
        "# test_model(model, test_loader, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "WOgyzow6Lwg_",
        "outputId": "83a9eedb-d4ff-40c9-85ea-8c90b476a1f3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-84e991d99ac1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Testing the model\n",
        "test_model(model, test_loader, dataset.vocab)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}